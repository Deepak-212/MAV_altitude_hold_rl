

## Overview

This project implements a Reinforcement Learning (RL) controller for aircraft altitude tracking.
The agent is trained to achieve accurate altitude tracking. To enhance safety and prevent stall conditions, Control Barrier Functions (CBFs) are integrated as a safety filter on the agent’s actions.

---

## State Space

The state vector provided to the RL agent is defined as:

S(t) = [
  (href - h) / 50,
  ∫(href - h) dt / 500,
  -w / 10,
  θ / 45°,
  q,
  (Va - 25) / 10
]

### State Variables

* (href): Target altitude
* (h): Current altitude
* (w): Vertical body-frame velocity
* (θ): Pitch angle
* (q): Pitch rate
* (Va): Airspeed

### Normalized Limits

Limits = [±∞, ±∞, ±∞, ±0.1, ±0.1, ±∞]

The integral of altitude error is explicitly clipped:

-500 ≤ ∫(href - h) dt ≤ 500

---

## Action Space

The action vector generated by the agent is:

A(t) = [ δe , δt ]

where:

* (δe): Elevator deflection command
* (δt): Throttle command

### Action Limits

-10° ≤ δe ≤ 10°
0 ≤ δt ≤ 1


---

## Reward Function

The reward function is designed to encourage:

* Accurate altitude tracking
* Smooth and efficient control inputs
* Safe airspeed (stall avoidance)
* Stable aircraft attitude

### 1) Altitude Tracking Reward

A Gaussian reward formulation is used:

R_alt = 2 * exp(-(href - h)^2 / (2σ^2))

σ = 5 m

---

### 2) Airspeed Penalty (Stall Avoidance)

If Va < Va_min:
    R_Va = -2 * (Va_min - Va)^2
Else:
    R_Va = 0

where:

* Va_min = 15 m/s

---

### 3) Smoothness and Control Effort Penalties

R_q = -0.05 * q^2

R_δe = -0.01 * δe^2

Throttle smoothness penalty:

R_Δδt = -0.5 * |δt(k) - δt(k-1)|

---

### 4) Pitch Angle Penalty

R_θ = -0.5 * θ^2


---

### 5) Survival Reward

R_survive = +2


---

### Total Reward

R = R_alt + R_Va + R_q + R_δe + R_Δδt + R_θ + R_survive

---

## Episode Termination Conditions

An episode terminates if any of the following conditions occur:

### 1) Ground Collision

h ≤ 0

R = -500

### 2) Excessive Altitude Deviation

|href - h| > 100 m

R = -500

### 3) Unsafe Pitch Angle

|θ| > 45°

R = -250

---

## Episode Truncation

An episode is truncated when the maximum episode length is reached.

---

## Safety via Control Barrier Functions (CBF)

To prevent stall and enhance safety, a Control Barrier Function (CBF) is used as a safety filter between the RL agent and the environment.

### Safe Set Definition

The safe set is defined by the barrier function:

h(x) = V - V_min


Safety is guaranteed when:

h(x) ≥ 0

---

### Safety Condition

To ensure the barrier function remains non-negative, the following condition is enforced:

ḣ + γh ≥ 0


where:

* (γ): barrier convergence rate

Since the system dynamics are affine in control:

ẋ = f(x) + g(x)u


the condition becomes:

Lf(h) + Lg(h) · u ≥ -γh


where:

* (Lf(h)): Lie derivative representing natural system dynamics
* (Lg(h)): Lie derivative representing control influence

---

### Barrier Function Derivatives

h(x) = V - V_min
ḣ = V̇

Lf(h) = (1 / (mV)) * [ u(fx,g + fx,aero) + v(fy,g + fy,aero) + w(fz,g + fz,aero) ]

Lg(h) = [ ∂V̇/∂δe , ∂V̇/∂δt ]

---

### Control Gains

#### 1) Elevator Gain

Lg(h)_δe = (q̄ / (mV)) * [ u * CX_δe(α) + w * CZ_δe(α) ]


#### 2) Throttle Gain

Assuming thrust is linear with throttle:

Lg(h)_δt = (u / (mV)) * Tmax


---

### Safe Action Constraint

The filtered safe action:
u_safe = [ δe_safe , δt_safe ]^T


must satisfy:

(Lg(h)_δe) * δe_safe + (Lg(h)_δt) * δt_safe ≥ -γ(V - V_min) - Lf(h)


This ensures the RL agent’s actions remain within the defined safety constraints.

---




